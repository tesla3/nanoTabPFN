# Session 2026-02-08: nanoTabPFN Attention Refactor & Eval Overhaul

## Changes Made

### 1. Attention Refactor (`model.py`)
- Removed `nn.MultiheadAttention` delegation for default+softmax case
- Unified all variants under single Q/K/V projection set (q_proj, k_proj, v_proj, out_proj)
- Default+softmax now uses `F.scaled_dot_product_attention` (SDPA fused kernel)
- Hypernetwork/non-softmax paths use manual `torch.matmul`/`torch.einsum`
- Q/K/V layout changed from `[B, S, H, D]` to `[B, H, S, D]` (standard for SDPA)
- Hypernetwork einsums updated: `"bhqk,bhkd->bqkd"` (was `"bhqk,bkhd->bqkd"` with old layout)
- Removed `MultiheadAttention` import, kept `Linear` and `LayerNorm`
- Note: model.py changes were amended into user's prior commit `fedb630`

### 2. ReLU → SiLU
- Replaced `F.relu` with `F.silu` in hypernetwork attention
- Renamed config key `mlp_relu` → `mlp_silu` everywhere
- Updated: `model.py`, `experiment_hypatt.py`, `experiment_hypatt_v2.py`, `test_attention_replacement.py`

### 3. Evaluation Overhaul
- Replaced sklearn toy datasets (breast_cancer, wine, iris, digits) with OpenML TabArena
- 51 TabArena task IDs, filtered to ~10 binary classification datasets
- Filters: max 20 features, 600 samples, binary only, no missing values, minority class ≥ 2.5%
- 5-fold stratified CV (StratifiedKFold, seed=0) instead of single 50/50 split
- ROC AUC only (dropped accuracy, balanced accuracy)
- Added per-dataset printing in `evaluate_model()`
- Fix: `.astype(np.float64)` after `fit_transform` (preprocessor returns object arrays)
- Same `get_feature_preprocessor()` from experiment.ipynb (handles categorical + numeric)

### 4. train.py
- Added argparse: `--num_steps` (default 2500), `--batch_size` (32), `--lr` (4e-3), `--steps_per_eval` (50)
- Added `get_openml_datasets()` function (same as experiment_hypatt.py)
- Renamed `eval()` → `eval_model()` to avoid shadowing builtin
- Added JSON result saving to `train_results.json` (config, n_params, final_metrics, history)
- `eval_model()` returns `{dataset_name}/ROC AUC` keys plus `ROC AUC` mean

### 5. Experiment Config Updates
- v1 (`experiment_hypatt.py`): User narrowed to rms_head + mlp_silu only
  - emb=512, heads=32, mlp=1024, 3 layers, num_outputs=2
  - LR sweep originally [1e-5..5e-4], user narrowed to [5e-5] for focused run
  - 2500 steps (bumped from 250), eval every 50
  - Note: arch_label says "emb256_16H" but actual config is emb=512, heads=32 (label mismatch from edit)
- v2 (`experiment_hypatt_v2.py`): Updated mlp_relu → mlp_silu in configs

### 6. Repo Cleanup & Infrastructure
- Installed `gh` CLI system-wide via apt (needed sudo)
- Created `tesla3/scale` private repo on GitHub
- Set up GitHub auth: `gh auth login` with personal access token, `gh auth setup-git`
- Replaced URM submodule with URM_fresh (force-pushed to tesla3/URM)
- Added CLAUDE.md to hypernetwork-attention and sata_attention submodules
- Set nanoTabPFN remote to `git@github.com:tesla3/nanoTabPFN.git` (was https://automl)
- All submodules pushed to tesla3 GitHub

## Experiment Results

### Baseline: default softmax attention (emb=96, heads=4, 2500 steps, lr=4e-3)
- `train_results.bench.json`: **mean ROC AUC = 0.744** (356K params)
- Best datasets: online_shoppers (0.900), churn (0.860), Bank_Churn (0.843)
- Worst: Is-this-a-good-customer (0.604), seismic-bumps (0.641)

### Baseline: default softmax attention (emb=96, heads=4, 500 steps, lr=4e-3)
- `train_results.json`: **mean ROC AUC = 0.715** (356K params)

### Hypernetwork: rms_head + mlp_silu (emb=512, heads=32, 2500 steps, lr=5e-5)
- `experiment_results.json`: **mean ROC AUC = 0.714** (10M params)
- Best datasets: diabetes (0.817), online_shoppers (0.818), Bank_Churn (0.806)
- Worst: Is-this-a-good-customer (0.527), Amazon_employee (0.607)

### User's standalone eval (train_result.json, likely 2500 steps default)
- **mean ROC AUC = 0.741** (appears to be default attention baseline)

### Key Observation
- Hypernetwork (10M params, 0.714) slightly underperforms baseline (356K params, 0.744)
- This is with a single LR (5e-5); full LR sweep may find better point
- The hypernetwork model is 28x larger but not better — may need different architecture tuning

## Files Modified/Created

### nanoTabPFN commits
1. `fedb630` Add HypernetMultiheadAttention (user amended with SDPA+SiLU changes)
2. `ef48b76` Replace sklearn eval with OpenML TabArena 5-fold CV, add experiment scripts
3. `a858240` Add CLI args and JSON result saving to train.py
4. `ff28700` Add CLAUDE.md and experiment result files

### Parent repo (scale) commits
1. `77dfefd` Update nanoTabPFN submodule: SDPA attention, SiLU, OpenML eval
2. `5dba16a` Add CLAUDE.md to hypernetwork-attention and sata_attention submodules
3. `04226a6` Replace URM submodule with URM_fresh (correct version)

## Validated
- All 18 attention variant tests pass (9 cross-attention + 9 self-attention)
- SDPA path (default+softmax): ~1e-7 max diff (float precision)
- All manual paths: exact 0.0 diff
- 500-step end-to-end train.py run successful

## OpenML Datasets Used (10 after filtering)
Amazon_employee_access, bank-marketing, Bank_Customer_Churn,
blood-transfusion-service-center, churn, diabetes,
E-CommereShippingData, Is-this-a-good-customer,
online_shoppers_intention, seismic-bumps
